{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP: Processing and Understanding Text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMLVFrnEKLhs9bpkspbgFw3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bjohn22/Natural-Language-Processing/blob/main/NLP_Processing_and_Understanding_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48YVlh8twYk_"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU0Bq6K9wlsh"
      },
      "source": [
        "To carry out different operations and analyze\n",
        "text, you will need to process and parse textual data into more easy-to-interpret formats.\n",
        "> All machine learning (ML) algorithms , be they supervised or unsupervised techniques, usually work with input features that are numeric in nature\n",
        "> Input text are usually raw and highly unstructured.\n",
        "\n",
        "Main objectives:\n",
        ">Clean, normalize and pre-process initial textual data\n",
        "\n",
        "Pre-processing :\n",
        "> using a variety of techniques to convert raw text into well defined\n",
        "sequences of linguistic components that have standard structure and notation.\n",
        "List of common tasks:\n",
        "1. Tokenization\n",
        "1. Tagging\n",
        "1. Chunking\n",
        "1. Stemming\n",
        "1. Lemmatization\n",
        "1. Misspelled text correction\n",
        "1. Stopwords removal\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8z6Q1afwl7V"
      },
      "source": [
        "# Text Tokenization\n",
        "tokens are independent and minimal textual components that have some definite syntax and semantics:\n",
        "> sentence and word tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sK64dKFwl-P"
      },
      "source": [
        "##Sentence Tokenization/ sentence segmentation\n",
        "splitting a text corpus into sentences that act as the first level of tokens which the corpus is comprised of.\n",
        "* Text corpus is a body of text where each paragraph comprises several sentences.\n",
        "Basic techniques:\n",
        "* look for specific delimiters between sentences, e.g. periods(.) or a newline character (\\n) and sometimes semi-colon (;).\n",
        "* from NLTK framework use any of:\n",
        "> * sent_tokenize\n",
        "> * PunktSentenceTokenizer\n",
        "> * RegexpTokenizer\n",
        "> * Pre-trained sentence tokenization models\n",
        "\n",
        "Let s see how these work!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_vHeOfxwmCK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyz6G4MH1bMF"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from pprint import pprint\n",
        "import re\n",
        "import string"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wawNirst2r3x",
        "outputId": "067601e0-4c3f-4ba6-d35a-07546bbf9319"
      },
      "source": [
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwed-VT41jRc"
      },
      "source": [
        "#load some sample text and also a part of the Gutenberg corpus available in NLTK itself.\n",
        "#Load some dependencies as well\n",
        "alice = gutenberg.raw(fileids='carroll-alice.txt')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGffSEXB2csO"
      },
      "source": [
        "sample_text = 'We will discuss briefly about the basic syntax, structure and design philosophies. There is a defined hierarchical syntax for Python code which you should remember when writing code! Python is a really powerful programming language!'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOSbi3he1jY3",
        "outputId": "c679fa6e-592d-4864-ce82-dd23cea65311"
      },
      "source": [
        "#check the length of the Alice in Wonderland corpus and also the first few lines\n",
        "# Total characters in Alice in Wonderland\n",
        "print('length is', len(alice),':', '1st 100 is', alice[0:100])\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length is 144395 : 1st 100 is [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
            "\n",
            "CHAPTER I. Down the Rabbit-Hole\n",
            "\n",
            "Alice was\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7uy4jEy38NR"
      },
      "source": [
        "nltk.sent_tokenize function is the default sentence tokenization function that\n",
        "nltk recommends. It uses an instance of the PunktSentenceTokenizer class internally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dvWkrYu1jcB",
        "outputId": "85ad4b7d-2b4d-4964-b6d3-bc6ffc132df2"
      },
      "source": [
        "default_st = nltk.sent_tokenize\n",
        "alice_sentences = default_st(text=alice)\n",
        "sample_sentences = default_st(text=sample_text)\n",
        "print ('Total sentences in sample_text:', len(sample_sentences))\n",
        "print ('Sample text sentences :-' )\n",
        "pprint(sample_sentences)\n",
        "print('\\nTotal sentences in alice:', len(alice_sentences))\n",
        "print ('First 5 sentences in alice:-')\n",
        "pprint(alice_sentences[0:5])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total sentences in sample_text: 3\n",
            "Sample text sentences :-\n",
            "['We will discuss briefly about the basic syntax, structure and design '\n",
            " 'philosophies.',\n",
            " 'There is a defined hierarchical syntax for Python code which you should '\n",
            " 'remember when writing code!',\n",
            " 'Python is a really powerful programming language!']\n",
            "\n",
            "Total sentences in alice: 1625\n",
            "First 5 sentences in alice:-\n",
            "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\",\n",
            " 'Down the Rabbit-Hole\\n'\n",
            " '\\n'\n",
            " 'Alice was beginning to get very tired of sitting by her sister on the\\n'\n",
            " 'bank, and of having nothing to do: once or twice she had peeped into the\\n'\n",
            " 'book her sister was reading, but it had no pictures or conversations in\\n'\n",
            " \"it, 'and what is the use of a book,' thought Alice 'without pictures or\\n\"\n",
            " \"conversation?'\",\n",
            " 'So she was considering in her own mind (as well as she could, for the\\n'\n",
            " 'hot day made her feel very sleepy and stupid), whether the pleasure\\n'\n",
            " 'of making a daisy-chain would be worth the trouble of getting up and\\n'\n",
            " 'picking the daisies, when suddenly a White Rabbit with pink eyes ran\\n'\n",
            " 'close by her.',\n",
            " 'There was nothing so VERY remarkable in that; nor did Alice think it so\\n'\n",
            " \"VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\",\n",
            " 'Oh dear!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdNqMrRh6UUS"
      },
      "source": [
        "The tokenizer is quite intelligent and doesn’t just use periods to\n",
        "delimit sentences. It also considers other punctuation and the capitalization of words ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3k5rmOZ1jfW",
        "outputId": "883ffc10-0b53-4392-907a-5a45c704a2a9"
      },
      "source": [
        "#Another tokenization with similar output as before.\n",
        "punkt_st = nltk.tokenize.PunktSentenceTokenizer()\n",
        "sample_sentences = punkt_st.tokenize(sample_text)\n",
        "pprint(sample_sentences)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['We will discuss briefly about the basic syntax, structure and design '\n",
            " 'philosophies.',\n",
            " 'There is a defined hierarchical syntax for Python code which you should '\n",
            " 'remember when writing code!',\n",
            " 'Python is a really powerful programming language!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghbNAUxO1jiW",
        "outputId": "ea6acf60-7645-46d5-ba6b-0ac23726969e"
      },
      "source": [
        "#Another tokenization using RegexpTokenizer with similar output as before.\n",
        "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
        "regex_st = nltk.tokenize.RegexpTokenizer(pattern=SENTENCE_TOKENS_PATTERN, gaps = True)\n",
        "sample_sentences = regex_st.tokenize(sample_text)\n",
        "pprint(sample_sentences)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['We will discuss briefly about the basic syntax, structure and design '\n",
            " 'philosophies.',\n",
            " 'There is a defined hierarchical syntax for Python code which you should '\n",
            " 'remember when writing code!',\n",
            " 'Python is a really powerful programming language!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCGBYwyS74VD"
      },
      "source": [
        "## Word Tokenization\n",
        "* process of splitting or segmenting sentences into their\n",
        "constituent words.\n",
        "* A sentence is a collection of words,\n",
        "> with tokenization we essentially split a sentence into a list of words that can be used to reconstruct the sentence.\n",
        "\n",
        "* Word tokenization is used in cleaning and normalizing\n",
        "> for stemming and lemmatization to be performed on individual word.\n",
        "\n",
        "From nltk package:\n",
        "* word_tokenize (default go to function and recommended)\n",
        "* TreebankWordTokenizer\n",
        "* RegexpTokenizer\n",
        "* Inherited tokenizers from RegexpTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1splcf27zUW",
        "outputId": "27d657ef-7b40-4a15-9fc3-474013764c69"
      },
      "source": [
        "#word tokenization for a single sentence\n",
        "sentence = \"The brown fox wasn't that quick and he couldn't win the race\"\n",
        "default_wt = nltk.word_tokenize\n",
        "words = default_wt(sentence)\n",
        "print(words)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLlr1mkV93n7"
      },
      "source": [
        "# Text Normalization (text cleansing or wrangling)\n",
        "Text normalization is defined as a process that consists of a series of steps that\n",
        "should be followed to wrangle, clean, and standardize textual data into a form that\n",
        "could be consumed by other NLP and analytics systems and applications as input.\n",
        "* techniques include: cleaning text, case conversion, correcting spellings,\n",
        "removing stopwords and other unnecessary terms, ***stemming, and lemmatization.***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujcay8aG7zbL"
      },
      "source": [
        "corpus = [\"The brown fox wasn't that quick and he couldn't win the race\",\n",
        "\"Hey that's a great deal! I just bought a phone for $199\",\n",
        "\"@@You'll (learn) a **lot** in the book. Python is an amazing language !@@\"]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH9IFYez-2cm"
      },
      "source": [
        "### Cleaning Text\n",
        "Often the textual data we want to use or analyze contains a lot of extraneous and unnecessary tokens and characters that should be removed before performing any\n",
        "further operations like tokenization or other normalization techniques. This includes\n",
        "*extracting out meaningful text from data sources like HTML data*, which consists of\n",
        "unnecessary HTML tags, or even data from XML and JSON feeds. There are many ways\n",
        "to parse and clean this data to remove unnecessary tags. You can use functions like\n",
        "***clean_html() from nltk or even the BeautifulSoup library to parse HTML data***. You can\n",
        "also use your own custom logic, including regexes, xpath, and the lxml library, to parse *italicized text*\n",
        "through XML data. And getting data from JSON is substantially easier because it has\n",
        "definite key-value annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgXWmv1p_nEw"
      },
      "source": [
        "## Tokenizing Text\n",
        "Usually, we tokenize text before or after removing unnecessary characters and symbols\n",
        "from the data. This choice depends on the problem you are trying to solve and the data\n",
        "you are dealing with.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Odmc_FV7zd8"
      },
      "source": [
        "#We will define a generic tokenization function here and run the same on our corpus mentioned earlier.\n",
        "#It takes in textual data, extracts sentences from it, and \n",
        "#finally splits each sentence into further tokens, which could be words or special characters and punctuation.\n",
        "\n",
        "def tokenize_text(text):\n",
        "  sentences = nltk.sent_tokenize(text)\n",
        "  word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "  return word_tokens"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjbqwOh17zj8",
        "outputId": "f095c1ac-e07f-4a24-e1e4-0f7e45583b97"
      },
      "source": [
        "token_list = [tokenize_text(text) for text in corpus]\n",
        "pprint(token_list)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[['The',\n",
            "   'brown',\n",
            "   'fox',\n",
            "   'was',\n",
            "   \"n't\",\n",
            "   'that',\n",
            "   'quick',\n",
            "   'and',\n",
            "   'he',\n",
            "   'could',\n",
            "   \"n't\",\n",
            "   'win',\n",
            "   'the',\n",
            "   'race']],\n",
            " [['Hey', 'that', \"'s\", 'a', 'great', 'deal', '!'],\n",
            "  ['I', 'just', 'bought', 'a', 'phone', 'for', '$', '199']],\n",
            " [['@',\n",
            "   '@',\n",
            "   'You',\n",
            "   \"'ll\",\n",
            "   '(',\n",
            "   'learn',\n",
            "   ')',\n",
            "   'a',\n",
            "   '**lot**',\n",
            "   'in',\n",
            "   'the',\n",
            "   'book',\n",
            "   '.'],\n",
            "  ['Python', 'is', 'an', 'amazing', 'language', '!'],\n",
            "  ['@', '@']]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHWjbq_5B2aW"
      },
      "source": [
        "## Removing Special Characters\n",
        "These may be special symbols or even punctuation that occurs in sentences.\n",
        "This step is often performed before or after tokenization. The main reason for doing so is\n",
        "because often punctuation or special characters do not have much significance when we\n",
        "analyze the text and utilize it for extracting features or information based on NLP and ML."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr-dJxBrAFE3"
      },
      "source": [
        "#remove special characters after tokenization:\n",
        "#what we do here is use the string.punctuation attribute, which consists of all possible special characters/symbols, and create a regex pattern from it.\n",
        "#We use it to match tokens that are symbols and characters and remove them.\n",
        "#The filter function helps us remove empty tokens obtained after removing the special character tokens using the regex sub method.\n",
        "def remove_characters_after_tokenization(tokens):\n",
        "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
        "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
        "    return filtered_tokens"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABWsDQ5rAFIJ",
        "outputId": "8dcaa04b-881b-496a-e248-c7b359be3565"
      },
      "source": [
        "filtered_list_1 = [filter(None,[remove_characters_after_tokenization(tokens) for tokens in sentence_tokens]) for sentence_tokens in token_list]\n",
        "print(filtered_list_1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<filter object at 0x7f488b9f6350>, <filter object at 0x7f488b9d5ed0>, <filter object at 0x7f488b9b6050>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDhl4bkHAFLU"
      },
      "source": [
        "#remove special characters before tokenization:\n",
        "def remove_characters_before_tokenization(sentence, keep_apostrophes=False):\n",
        "    sentence = sentence.strip()\n",
        "    if keep_apostrophes:\n",
        "      PATTERN = r'[?|$|&|*|%|@|(|)|~]' # add other characters here to remove them\n",
        "      filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
        "    else:\n",
        "        PATTERN = r'[^a-zA-Z0-9 ]' # only extract alpha-numeric characters\n",
        "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
        "    return filtered_sentence"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWMh5N44AFN7",
        "outputId": "623f97d6-9305-492d-df5c-3562a3c6c94b"
      },
      "source": [
        "filtered_list_2 = [remove_characters_before_tokenization(sentence) for sentence in corpus]\n",
        "print (filtered_list_2)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The brown fox wasnt that quick and he couldnt win the race', 'Hey thats a great deal I just bought a phone for 199', 'Youll learn a lot in the book Python is an amazing language ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tomid5V_AFQ5",
        "outputId": "4dcba816-86bf-4f05-f241-eaedd92a38b7"
      },
      "source": [
        "cleaned_corpus = [remove_characters_before_tokenization(sentence, keep_apostrophes=True) for sentence in corpus]\n",
        "print (cleaned_corpus)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"The brown fox wasn't that quick and he couldn't win the race\", \"Hey that's a great deal! I just bought a phone for 199\", \"You'll learn a lot in the book. Python is an amazing language !\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFs-yda7FdO4"
      },
      "source": [
        "The preceding outputs show two different ways of removing special characters before\n",
        "tokenization—removing all special characters versus retaining apostrophes and sentence\n",
        "periods—using regular expressions. By now, you must have realized how powerful regular\n",
        "expressions can be, as mentioned in Chapter 2 . Usually after removing these characters,\n",
        "you can take the clean text and tokenize it or apply other normalization operations on it.\n",
        "Sometimes we want to preserve the apostrophes in the sentences as a way to track them\n",
        "and expand them if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1rLMomkAbcE"
      },
      "source": [
        "####Expanding Contractions\n",
        "* Contractions are shortened version of words or syllables\n",
        "* Examples would be `is not` to `isn’t` and `will not` to `won’t`\n",
        "\n",
        "By nature, contractions do pose a problem for NLP and text analytics because, to\n",
        "start with, we have a special apostrophe character in the word. Plus we have two or more words represented by a contraction, and this opens a whole new can of worms when we try to tokenize this or even standardize the words.\n",
        "\n",
        "###### Approach to dealing with contractions\n",
        "* have a proper mapping for contractions and their corresponding expansions\n",
        "* then use it to expand all the contractions in the text\n",
        "* I have created a vocabulary for contractions and their corresponding expanded forms that you can access in the file **`contractions.py`** in a Python dictionary (available along with the code files for this chapter). Part of the contractions dictionary is shown below\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD0SS3g3CfNz"
      },
      "source": [
        "CONTRACTION_MAP = {\n",
        "\"isn't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"that's\": \"that is\",\n",
        "\"you'll\": \"you will\"\n",
        "}"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "z4pRumI6I21l",
        "outputId": "cf28e6ae-628b-4b3e-879b-b6201bf638b8"
      },
      "source": [
        "import CONTRACTION_MAP"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-f7cee9f6e60d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mCONTRACTION_MAP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'CONTRACTION_MAP'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9JiOuE3Fo08"
      },
      "source": [
        "* uses the function `expanded_match` inside the main `expand_contractions` function to find each contraction that matches the regex pattern we\n",
        "created out of all the contractions in our ***CONTRACTION_MAP*** dictionary. On matching any contraction, we substitute it with its corresponding expanded version and retain the correct case of the word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cSakge0DSGz"
      },
      "source": [
        "#Implementing the contractions map\n",
        "\n",
        "def expand_contractions(sentence, contraction_mapping):\n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
        "    flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contraction_mapping.get(match)\\\n",
        "                              if contraction_mapping.get(match)\\\n",
        "                              else contraction_mapping.get(match.lower())\n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
        "    return expanded_sentence\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x68Pw8gDSVp",
        "outputId": "da499e72-7680-41d3-c478-e7896bbdfb7b"
      },
      "source": [
        "expanded_corpus = [expand_contractions(sentence, CONTRACTION_MAP) for sentence in cleaned_corpus]\n",
        "expanded_corpus"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The brown fox was not that quick and he could not win the race',\n",
              " 'Hey that is a great deal! I just bought a phone for 199',\n",
              " 'You will learn a lot in the book. Python is an amazing language !']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXR2KLpw0-kj"
      },
      "source": [
        "#### Removing Stopwords\n",
        "* These words have litle or no significance\n",
        ">They are usually removed from text during processing so as to retain words having maximum significance and context. Stopwords are usually words that end up occurring the most if you aggregated any corpus of text based on singular tokens and checked their frequencies. Words like *a, the , me ,* and so on are stopwords.\n",
        "\n",
        "* One important thing to remember is that negations like `not` and `no` are removed in this case \n",
        "> it is often essential to preserve them so the actual context of the sentence is not lost in applications like *sentiment analysis*, \n",
        "> so you would need to make sure ***you do not remove such words in those scenarios***."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVjAbYuyKCDk",
        "outputId": "5311f9de-b0df-43fb-8f25-7b6b220f27fb"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nD7McwJLD9O",
        "outputId": "4545d57f-0d01-435e-832f-722031cd4eb7"
      },
      "source": [
        "#See the list of stopwords in nltk's vocabulary\n",
        "nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E4RSLC37zmX"
      },
      "source": [
        "#we leverage the use of nltk , which has a list of stopwords for English,\n",
        "#and use it to filter out all tokens that correspond to stopwords.\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    stopword_list = nltk.corpus.stopwords.words('english')\n",
        "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    return filtered_tokens"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAK-I8F0-cDt"
      },
      "source": [
        "expanded_corpus_tokens = [tokenize_text(text)\n",
        "                          for text in expanded_corpus]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0_qjfH_-caJ",
        "outputId": "3b89f6e9-bd6b-4085-bea2-04b0fdb76113"
      },
      "source": [
        "filtered_list_3 = [[remove_stopwords(tokens)\n",
        "                  for tokens in sentence_tokens]\n",
        "                  for sentence_tokens in expanded_corpus_tokens\n",
        "                   ]\n",
        "filtered_list_3"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['The', 'brown', 'fox', 'quick', 'could', 'win', 'race']],\n",
              " [['Hey', 'great', 'deal', '!'], ['I', 'bought', 'phone', '199']],\n",
              " [['You', 'learn', 'lot', 'book', '.'],\n",
              "  ['Python', 'amazing', 'language', '!']]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tusuf8YL-cpZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwE09o0x-c1z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wexlim940-dB"
      },
      "source": [
        "#### Stemming\n",
        "* We start with explaining morphemes:\n",
        "* Morphemes are the smallest independent unit in any natural language\n",
        "* Morphemes consist of units that are stems and affixes.\n",
        "* Affixes are units like prefixes, suffixes, and so on, which are attached to a word stem to change its meaning or create a new word altogether.\n",
        "* Word stems are also often known as the *base form* of a word, and we can create new words by attaching affixes to them in a process known as *inflection*.\n",
        "* The reverse of this is obtaining the base form of a word from its inflected form, and this is known as ***stemming***.\n",
        "* The *nltk* package has several implementations for stemmers. These stemmers are implemented in the *stem module*, which inherits the *StemmerI* interface in the *nltk.stem.api* module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q9e3qfi1HOO"
      },
      "source": [
        "# Porter Stemmer\n",
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY55sT8TMa2W"
      },
      "source": [
        "ps = PorterStemmer()"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HyGYB890-z0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt0msSNW1jqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "315ed499-efe6-4aa2-c221-9eb1d874de5a"
      },
      "source": [
        "print (ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped'))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jump jump jump\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjElrTe_MkbZ",
        "outputId": "97027deb-770e-495a-ee4b-0c6d898cef38"
      },
      "source": [
        "print (ps.stem('lying'), ps.stem('strange'))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lie strang\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeKVEfjwMkfl"
      },
      "source": [
        "#Alternative stop words with different result\n",
        "\n",
        "from nltk.stem import LancasterStemmer\n",
        "ls = LancasterStemmer()"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ3f5A0WMkjN",
        "outputId": "8e7f8b97-ecbe-40db-9c77-c581cc4bf785"
      },
      "source": [
        "print(ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped'), ls.stem('lying'), ls.stem('strange'))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jump jump jump lying strange\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4X5TAjTMkm9"
      },
      "source": [
        "#Yet another alternative stop words with different result\n",
        "# Snowball Stemmer\n",
        "from nltk.stem import SnowballStemmer"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2AHzKDNMkqI"
      },
      "source": [
        "ss = SnowballStemmer(\"english\")"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCTJE1u8NXKl",
        "outputId": "f0952af0-1636-4abd-d6e7-e02d5b79e052"
      },
      "source": [
        "print(ss.stem('jumping'), ss.stem('jumps'), ss.stem('jumped'), ss.stem('lying'), ss.stem('strange'))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jump jump jump lie strang\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anVWwTyfNqO_"
      },
      "source": [
        "####Lemmatization\n",
        "* The process of *lemmatization* is very similar to stemming—you remove word affixes to get to a base form of the word. But in this case, this base form is also known as the *root word*, but not the *root stem*. \n",
        "* The difference is that the root stem may not always be a lexicographically correct word; that is, it may not be present in the dictionary. The *root word*, also known as the *lemma*, will always be present in the dictionary.\n",
        "\n",
        "\n",
        "The lemmatization process is considerably slower than stemming because an\n",
        "additional step is involved where the root form or lemma is formed by removing the affix from the word if and only if the lemma is present in the dictionary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxyuGeZlNXUk"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf7ItCSlPyji",
        "outputId": "7f891592-f386-4775-9e95-8ca5be183137"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73hjRBs_NXXl"
      },
      "source": [
        "wnl = WordNetLemmatizer()"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n1jFimiNXb4",
        "outputId": "8288d868-e8e8-49bc-844d-129faf10e0ea"
      },
      "source": [
        "# lemmatize nouns\n",
        "print (wnl.lemmatize('cars', 'n'), wnl.lemmatize('houses', 'n'))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "car house\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idYwfneCMktL",
        "outputId": "1ca89ca1-56b3-46d6-91f2-1625dde45d0a"
      },
      "source": [
        "# lemmatize verbs\n",
        "print (wnl.lemmatize('running', 'v'), wnl.lemmatize('ate', 'v'), wnl.lemmatize('thought', 'v'))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run eat think\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDjl_p7DMkwc",
        "outputId": "1e58f5d2-5e21-4878-863c-40ab21f25bec"
      },
      "source": [
        "# lemmatize adjectives\n",
        "print (wnl.lemmatize('saddest', 'a'), wnl.lemmatize('better', 'a'), wnl.lemmatize('tallest', 'a'), wnl.lemmatize('fancier', 'a'))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sad good tall fancy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVt-dlzpQSMU"
      },
      "source": [
        "The preceding code leverages the `WordNetLemmatizer` class , which internally uses the `morphy()` function belonging to the `WordNetCorpusReader` class. This function basically finds the base form or lemma for a given word using the word and its part of speech by checking the Wordnet corpus and\n",
        "uses a recursive technique for removing affixes from the word until a match is found in WordNet. If no match is found, the input word itself is returned unchanged.\n",
        "* ***The part of speech is extremely important here because if that is wrong, the lemmatization will not be effective.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPfxdG2xMkzv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}